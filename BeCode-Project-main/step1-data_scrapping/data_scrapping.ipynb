{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from threading import RLock\n",
    "from threading import Thread\n",
    "from time import perf_counter\n",
    "import xmltodict\n",
    "\n",
    "start_urlscollection = perf_counter()\n",
    "print(\"Starting now to collect the urls...\")\n",
    "\n",
    "# Creating a list of all listings for houses and apartments for sale in Belgium  !! 130.000+ listings\n",
    "main_url=['https://assets.immoweb.be/sitemap/classifieds-'+'{0:03}'.format(i)+'.xml' for i in range(30)]\n",
    "\n",
    "def append_list(propertieslist,obj):\n",
    "    i = 2\n",
    "    while i < len(obj['urlset']['url']):\n",
    "        if \"house/for-sale\" in obj['urlset']['url'][i]['loc'] or \"apartment/for-sale\" in obj['urlset']['url'][i]['loc']:\n",
    "            propertieslist.append(obj['urlset']['url'][i]['loc'])\n",
    "        i += 3\n",
    "\n",
    "def get_list():\n",
    "    propertieslist=[]\n",
    "    for xml_url in main_url:\n",
    "        r = requests.get(xml_url, allow_redirects=True)\n",
    "        obj=xmltodict.parse(r.content)\n",
    "        append_list(propertieslist,obj)\n",
    "        print(xml_url+' has been scrapped')\n",
    "    return list(dict.fromkeys(propertieslist))\n",
    "\n",
    "# Saving the urls in a separate .txt file for future use and check\n",
    "if __name__ == \"__main__\":\n",
    "    propertieslist=get_list()\n",
    "    with open('Property_urls.txt', 'w') as fp:\n",
    "        for item in propertieslist:\n",
    "        # write each item on a new line\n",
    "            fp.write(\"%s\\n\" % item)\n",
    "\n",
    "print(\"Collection of urls finished.\")\n",
    "\n",
    "print(\"Number of urls collected: \" + str(len(propertieslist))) # Checking the amount of results returned - must be >10.000 as per requierement in the project\n",
    "\n",
    "end_urlscollection  = perf_counter()\n",
    "print(\"Urls collection process time: \" + str(end_urlscollection  - start_urlscollection))\n",
    "\n",
    "# Moving on to the scraping part\n",
    "start_scraping = perf_counter()\n",
    "print(\"Starting now to scrape the pages...\")\n",
    "\n",
    "# Creating a dictionnary to store the informations collected\n",
    "data = list()\n",
    "\n",
    "lock = RLock()\n",
    "\n",
    "def collect_infos(url):\n",
    "\n",
    "    with lock:\n",
    "\n",
    "        print(url) # To be removed once program is up and running\n",
    "\n",
    "        # Getting all the property specific page content as a workable json format (with help from Beautifulsoup)\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")               # \"lxml\" fast than \"html.parser\" - pip install lxml\n",
    "        content = soup.find('div', class_=\"container-main-content\")\n",
    "        try:\n",
    "            content_string = content.script.string\n",
    "            content_span = re.search(' = ', content_string).span()[1]\n",
    "            content_sliced = content_string[content_span:-10]\n",
    "            info = json.loads(content_sliced)\n",
    "\n",
    "            # Collecting the different informations on the property\n",
    "            immoweb_code = info[\"id\"]\n",
    "            locality = info[\"property\"][\"location\"][\"locality\"]\n",
    "            type = info[\"property\"][\"type\"]\n",
    "            subtype = info[\"property\"][\"subtype\"]\n",
    "            price = info[\"transaction\"][\"sale\"][\"price\"]\n",
    "            sale_type = info[\"transaction\"][\"subtype\"]\n",
    "            bedrooms = info[\"property\"][\"bedroomCount\"]\n",
    "            living_area = info[\"property\"][\"netHabitableSurface\"]\n",
    "            if info[\"property\"][\"kitchen\"] is None:\n",
    "                kitchen = \"None\"\n",
    "            else:\n",
    "                kitchen = \"1\" if info[\"property\"][\"kitchen\"][\"type\"] == \"HYPER_EQUIPPED\" else \"0\"\n",
    "            if info[\"transaction\"][\"sale\"] is not None and info[\"transaction\"][\"sale\"][\"isFurnished\"] == \"true\":\n",
    "                furnished = \"1\"\n",
    "            else:\n",
    "                furnished = \"None\"\n",
    "            open_fire = \"None\" if info[\"property\"][\"fireplaceExists\"] is None else int(info[\"property\"][\"fireplaceExists\"])\n",
    "            terrace = \"None\" if info[\"property\"][\"hasTerrace\"] is None else int(info[\"property\"][\"hasTerrace\"])\n",
    "            area_terrace = \"None\" if info[\"property\"][\"terraceSurface\"] is None else info[\"property\"][\"terraceSurface\"]\n",
    "            garden = \"None\" if info[\"property\"][\"hasGarden\"] is None else int(info[\"property\"][\"hasGarden\"])\n",
    "            area_garden = \"None\" if info[\"property\"][\"gardenSurface\"] is None else info[\"property\"][\"gardenSurface\"]\n",
    "            land_surface = \"None\" if info[\"property\"][\"land\"] is None else info[\"property\"][\"land\"][\"surface\"]\n",
    "            facades = \"None\" if info[\"property\"][\"building\"] is None else info[\"property\"][\"building\"][\"facadeCount\"]\n",
    "            swimming_pool = \"None\" if info[\"property\"][\"hasSwimmingPool\"] is None else int(info[\"property\"][\"hasSwimmingPool\"])\n",
    "            building_state = \"None\" if info[\"property\"][\"building\"] is None else info[\"property\"][\"building\"][\"condition\"] \n",
    "\n",
    "            # Adding the informations collected to the dictionnary\n",
    "            infos = {\n",
    "                \"Id\": immoweb_code, \"Locality\": locality, \"House or appartment?\": type, \"Subtype\": subtype, \n",
    "                \"Price\": price , \"Type of sale\": sale_type, \"Number of rooms\": bedrooms, \"Living area\": living_area, \n",
    "                \"Fully equipped kitchen\": kitchen , \"Furnished\": furnished , \"Open fire\": open_fire, \"Terrace\": terrace, \n",
    "                \"Area of terrace\": area_terrace, \"Garden\": garden, \"Area of garden\": area_garden , \"Land surface\": land_surface ,\n",
    "                \"Number of facades\": facades , \"Swimming pool\": swimming_pool, \"State of building\": building_state\n",
    "                }\n",
    "\n",
    "            data.append(infos)\n",
    "\n",
    "        except:\n",
    "            print(\"One property removed from the dataset due to errors.\\n\")\n",
    "            pass\n",
    "\n",
    "# Creating a list of sublists for the propertieslist of the size of the CPU count on the computer to optimize their use\n",
    "properties_array = np.array(random.sample(propertieslist, 15000))       # Limiting list to 15.000 entries randomly picked as list has >130.000 entries\n",
    "properties_split = np.array_split(properties_array, os.cpu_count())\n",
    "propertieslistofsublists = [list(subarray) for subarray in properties_split]\n",
    "\n",
    "def collect_infos_sublist(sublist):\n",
    "    for link in sublist:\n",
    "        collect_infos(link)\n",
    "\n",
    "# Using a thread to collect simultaneously the urls on all pages\n",
    "threads = list()\n",
    "\n",
    "for sublist in propertieslistofsublists:\n",
    "    thread = Thread(target=collect_infos_sublist, args=(sublist,))\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print(\"Scraping of pages finished.\")\n",
    "\n",
    "print(\"Number of pages scraped: \" + str(len(data)))\n",
    "\n",
    "end_scraping = perf_counter()\n",
    "print(\"Scraping process time: \" + str(end_scraping - start_scraping))\n",
    "\n",
    "# Copying the data collected into a .csv file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Immoweb_data.csv', index=False, header=True)\n",
    "\n",
    "print(\"Full program finished. Data can be found in Immoweb_data.csv file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b671cd3297ce6de26db2f8fd5e971e20d876a82d9da5776a1d4f9c2f2b55539"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
